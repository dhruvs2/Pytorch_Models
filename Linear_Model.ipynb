{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,tensor\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering Y = X*W (Simplicity)\n",
    "\n",
    "#random data\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0,4.0,6.0]\n",
    "\n",
    "w = 1.0 #random guess initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model forward pass\n",
    "def forward(x):\n",
    "    return x*w\n",
    "    \n",
    "#Loss function: L = (x*w - y)^2\n",
    "def losses(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)*(y_pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4.0 0.0 16.0\n",
      "\t 3.0 6.0 0.0 36.0\n",
      "MSE =  18.666666666666668\n",
      "w = 0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4.0 0.2 14.44\n",
      "\t 3.0 6.0 0.30000000000000004 32.49\n",
      "MSE =  16.846666666666668\n",
      "w = 0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4.0 0.4 12.96\n",
      "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
      "MSE =  15.120000000000003\n",
      "w = 0.30000000000000004\n",
      "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
      "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
      "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
      "MSE =  13.486666666666665\n",
      "w = 0.4\n",
      "\t 1.0 2.0 0.4 2.5600000000000005\n",
      "\t 2.0 4.0 0.8 10.240000000000002\n",
      "\t 3.0 6.0 1.2000000000000002 23.04\n",
      "MSE =  11.946666666666667\n",
      "w = 0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4.0 1.0 9.0\n",
      "\t 3.0 6.0 1.5 20.25\n",
      "MSE =  10.5\n",
      "w = 0.6000000000000001\n",
      "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
      "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
      "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
      "MSE =  9.146666666666663\n",
      "w = 0.7000000000000001\n",
      "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
      "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
      "\t 3.0 6.0 2.1 15.209999999999999\n",
      "MSE =  7.886666666666666\n",
      "w = 0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4.0 1.6 5.76\n",
      "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
      "MSE =  6.719999999999999\n",
      "w = 0.9\n",
      "\t 1.0 2.0 0.9 1.2100000000000002\n",
      "\t 2.0 4.0 1.8 4.840000000000001\n",
      "\t 3.0 6.0 2.7 10.889999999999999\n",
      "MSE =  5.646666666666666\n",
      "w = 1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4.0 2.0 4.0\n",
      "\t 3.0 6.0 3.0 9.0\n",
      "MSE =  4.666666666666667\n",
      "w = 1.1\n",
      "\t 1.0 2.0 1.1 0.8099999999999998\n",
      "\t 2.0 4.0 2.2 3.2399999999999993\n",
      "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
      "MSE =  3.779999999999999\n",
      "w = 1.2000000000000002\n",
      "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
      "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
      "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
      "MSE =  2.986666666666665\n",
      "w = 1.3\n",
      "\t 1.0 2.0 1.3 0.48999999999999994\n",
      "\t 2.0 4.0 2.6 1.9599999999999997\n",
      "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
      "MSE =  2.2866666666666657\n",
      "w = 1.4000000000000001\n",
      "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
      "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
      "\t 3.0 6.0 4.2 3.2399999999999993\n",
      "MSE =  1.6799999999999995\n",
      "w = 1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4.0 3.0 1.0\n",
      "\t 3.0 6.0 4.5 2.25\n",
      "MSE =  1.1666666666666667\n",
      "w = 1.6\n",
      "\t 1.0 2.0 1.6 0.15999999999999992\n",
      "\t 2.0 4.0 3.2 0.6399999999999997\n",
      "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
      "MSE =  0.746666666666666\n",
      "w = 1.7000000000000002\n",
      "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
      "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
      "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
      "MSE =  0.4199999999999995\n",
      "w = 1.8\n",
      "\t 1.0 2.0 1.8 0.03999999999999998\n",
      "\t 2.0 4.0 3.6 0.15999999999999992\n",
      "\t 3.0 6.0 5.4 0.3599999999999996\n",
      "MSE =  0.1866666666666665\n",
      "w = 1.9000000000000001\n",
      "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
      "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
      "\t 3.0 6.0 5.7 0.0899999999999999\n",
      "MSE =  0.046666666666666586\n",
      "w = 2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4.0 4.0 0.0\n",
      "\t 3.0 6.0 6.0 0.0\n",
      "MSE =  0.0\n",
      "w = 2.1\n",
      "\t 1.0 2.0 2.1 0.010000000000000018\n",
      "\t 2.0 4.0 4.2 0.04000000000000007\n",
      "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
      "MSE =  0.046666666666666835\n",
      "w = 2.2\n",
      "\t 1.0 2.0 2.2 0.04000000000000007\n",
      "\t 2.0 4.0 4.4 0.16000000000000028\n",
      "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
      "MSE =  0.18666666666666698\n",
      "w = 2.3000000000000003\n",
      "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
      "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
      "\t 3.0 6.0 6.9 0.8100000000000006\n",
      "MSE =  0.42000000000000054\n",
      "w = 2.4000000000000004\n",
      "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
      "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
      "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
      "MSE =  0.7466666666666679\n",
      "w = 2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4.0 5.0 1.0\n",
      "\t 3.0 6.0 7.5 2.25\n",
      "MSE =  1.1666666666666667\n",
      "w = 2.6\n",
      "\t 1.0 2.0 2.6 0.3600000000000001\n",
      "\t 2.0 4.0 5.2 1.4400000000000004\n",
      "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
      "MSE =  1.6800000000000008\n",
      "w = 2.7\n",
      "\t 1.0 2.0 2.7 0.49000000000000027\n",
      "\t 2.0 4.0 5.4 1.960000000000001\n",
      "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
      "MSE =  2.2866666666666693\n",
      "w = 2.8000000000000003\n",
      "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
      "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
      "\t 3.0 6.0 8.4 5.760000000000002\n",
      "MSE =  2.986666666666668\n",
      "w = 2.9000000000000004\n",
      "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
      "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
      "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
      "MSE =  3.780000000000003\n",
      "w = 3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4.0 6.0 4.0\n",
      "\t 3.0 6.0 9.0 9.0\n",
      "MSE =  4.666666666666667\n",
      "w = 3.1\n",
      "\t 1.0 2.0 3.1 1.2100000000000002\n",
      "\t 2.0 4.0 6.2 4.840000000000001\n",
      "\t 3.0 6.0 9.3 10.890000000000004\n",
      "MSE =  5.646666666666668\n",
      "w = 3.2\n",
      "\t 1.0 2.0 3.2 1.4400000000000004\n",
      "\t 2.0 4.0 6.4 5.760000000000002\n",
      "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
      "MSE =  6.720000000000003\n",
      "w = 3.3000000000000003\n",
      "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
      "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
      "\t 3.0 6.0 9.9 15.210000000000003\n",
      "MSE =  7.886666666666668\n",
      "w = 3.4000000000000004\n",
      "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
      "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
      "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
      "MSE =  9.14666666666667\n",
      "w = 3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4.0 7.0 9.0\n",
      "\t 3.0 6.0 10.5 20.25\n",
      "MSE =  10.5\n",
      "w = 3.6\n",
      "\t 1.0 2.0 3.6 2.5600000000000005\n",
      "\t 2.0 4.0 7.2 10.240000000000002\n",
      "\t 3.0 6.0 10.8 23.040000000000006\n",
      "MSE =  11.94666666666667\n",
      "w = 3.7\n",
      "\t 1.0 2.0 3.7 2.8900000000000006\n",
      "\t 2.0 4.0 7.4 11.560000000000002\n",
      "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
      "MSE =  13.486666666666673\n",
      "w = 3.8000000000000003\n",
      "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
      "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
      "\t 3.0 6.0 11.4 29.160000000000004\n",
      "MSE =  15.120000000000005\n",
      "w = 3.9000000000000004\n",
      "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
      "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
      "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
      "MSE =  16.84666666666667\n",
      "w = 4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4.0 8.0 16.0\n",
      "\t 3.0 6.0 12.0 36.0\n",
      "MSE =  18.666666666666668\n"
     ]
    }
   ],
   "source": [
    "w_list = []\n",
    "MSE_list = []\n",
    "for w in np.arange(0.0,4.1,0.1):\n",
    "    print(\"w =\", w)\n",
    "    loss_sum = 0\n",
    "    for x_val,y_val in zip(x_data,y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = losses(x_val,y_val)\n",
    "        loss_sum += l\n",
    "        print(\"\\t\", x_val,y_val,y_pred_val,l)\n",
    "        \n",
    "    print(\"MSE = \", loss_sum / 3) #Avg for three values\n",
    "    w_list.append(w)\n",
    "    MSE_list.append(loss_sum / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w_list,MSE_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient computation\n",
    "\n",
    "def gradient(x,y):\n",
    "    return 2*x*(x*w - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict(before training) 4 16.0\n"
     ]
    }
   ],
   "source": [
    "#Before training\n",
    "\n",
    "print(\"predict(before training)\", 4, forward(4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  1.0 2.0 2.993161274389422e-13\n",
      "\tgrad:  2.0 4.0 1.1723955140041653e-12\n",
      "\tgrad:  3.0 6.0 2.4300561562995426e-12\n",
      "progress: 0 w =  2.0000000000001106 loss =  1.1034270797869425e-25\n",
      "\tgrad:  1.0 2.0 2.2115642650533118e-13\n",
      "\tgrad:  2.0 4.0 8.668621376273222e-13\n",
      "\tgrad:  3.0 6.0 1.7905676941154525e-12\n",
      "progress: 1 w =  2.0000000000000817 loss =  6.00922683161158e-26\n",
      "\tgrad:  1.0 2.0 1.6342482922482304e-13\n",
      "\tgrad:  2.0 4.0 6.394884621840902e-13\n",
      "\tgrad:  3.0 6.0 1.326938559031987e-12\n",
      "progress: 2 w =  2.0000000000000604 loss =  3.282923543167763e-26\n",
      "\tgrad:  1.0 2.0 1.2079226507921703e-13\n",
      "\tgrad:  2.0 4.0 4.725109192804666e-13\n",
      "\tgrad:  3.0 6.0 9.752199048307375e-13\n",
      "progress: 3 w =  2.0000000000000444 loss =  1.7749370367472766e-26\n",
      "\tgrad:  1.0 2.0 8.881784197001252e-14\n",
      "\tgrad:  2.0 4.0 3.481659405224491e-13\n",
      "\tgrad:  3.0 6.0 7.194245199571014e-13\n",
      "progress: 4 w =  2.000000000000033 loss =  9.719555213228086e-27\n",
      "\tgrad:  1.0 2.0 6.572520305780927e-14\n",
      "\tgrad:  2.0 4.0 2.5934809855243657e-13\n",
      "\tgrad:  3.0 6.0 5.329070518200751e-13\n",
      "progress: 5 w =  2.0000000000000244 loss =  5.3043007267060834e-27\n",
      "\tgrad:  1.0 2.0 4.884981308350689e-14\n",
      "\tgrad:  2.0 4.0 1.9184653865522705e-13\n",
      "\tgrad:  3.0 6.0 3.9968028886505635e-13\n",
      "progress: 6 w =  2.000000000000018 loss =  3.0323813196695694e-27\n",
      "\tgrad:  1.0 2.0 3.6415315207705135e-14\n",
      "\tgrad:  2.0 4.0 1.4210854715202004e-13\n",
      "\tgrad:  3.0 6.0 2.984279490192421e-13\n",
      "progress: 7 w =  2.0000000000000133 loss =  1.5974433330725489e-27\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-14\n",
      "\tgrad:  2.0 4.0 1.0302869668521453e-13\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-13\n",
      "progress: 8 w =  2.0000000000000098 loss =  8.590695257856819e-28\n",
      "\tgrad:  1.0 2.0 1.9539925233402755e-14\n",
      "\tgrad:  2.0 4.0 7.815970093361102e-14\n",
      "\tgrad:  3.0 6.0 1.5987211554602254e-13\n",
      "progress: 9 w =  2.000000000000007 loss =  4.543838814073028e-28\n",
      "\tgrad:  1.0 2.0 1.4210854715202004e-14\n",
      "\tgrad:  2.0 4.0 5.684341886080802e-14\n",
      "\tgrad:  3.0 6.0 1.1723955140041653e-13\n",
      "progress: 10 w =  2.0000000000000053 loss =  2.5559093329160782e-28\n",
      "\tgrad:  1.0 2.0 1.0658141036401503e-14\n",
      "\tgrad:  2.0 4.0 4.263256414560601e-14\n",
      "\tgrad:  3.0 6.0 8.526512829121202e-14\n",
      "progress: 11 w =  2.000000000000004 loss =  1.5461673742331831e-28\n",
      "\tgrad:  1.0 2.0 7.993605777301127e-15\n",
      "\tgrad:  2.0 4.0 3.197442310920451e-14\n",
      "\tgrad:  3.0 6.0 6.394884621840902e-14\n",
      "progress: 12 w =  2.000000000000003 loss =  7.888609052210118e-29\n",
      "\tgrad:  1.0 2.0 6.217248937900877e-15\n",
      "\tgrad:  2.0 4.0 2.4868995751603507e-14\n",
      "\tgrad:  3.0 6.0 4.796163466380676e-14\n",
      "progress: 13 w =  2.000000000000002 loss =  5.048709793414476e-29\n",
      "\tgrad:  1.0 2.0 4.440892098500626e-15\n",
      "\tgrad:  2.0 4.0 1.7763568394002505e-14\n",
      "\tgrad:  3.0 6.0 4.263256414560601e-14\n",
      "progress: 14 w =  2.0000000000000018 loss =  2.8398992587956425e-29\n",
      "\tgrad:  1.0 2.0 3.552713678800501e-15\n",
      "\tgrad:  2.0 4.0 1.4210854715202004e-14\n",
      "\tgrad:  3.0 6.0 3.197442310920451e-14\n",
      "progress: 15 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 16 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 17 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 18 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 19 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 20 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 21 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 22 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 23 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 24 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 25 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 26 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 27 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 28 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 29 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 30 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 31 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 32 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 33 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 34 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 35 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 36 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 37 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 38 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 39 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 40 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 41 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 42 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 43 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 44 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 45 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 46 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 47 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 48 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 49 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 50 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 51 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 52 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 53 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 54 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 55 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 56 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 57 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 58 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 59 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 60 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 61 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 62 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 63 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 64 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 65 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 66 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 67 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 68 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 69 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 70 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 71 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 72 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 73 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 74 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 75 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 76 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 77 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 78 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 79 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 80 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 81 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 82 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 83 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 84 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 85 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 86 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 87 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 88 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 89 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 90 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 91 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 92 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 93 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 94 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 95 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 96 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 97 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 98 w =  2.0000000000000013 loss =  1.262177448353619e-29\n",
      "\tgrad:  1.0 2.0 2.6645352591003757e-15\n",
      "\tgrad:  2.0 4.0 1.0658141036401503e-14\n",
      "\tgrad:  3.0 6.0 2.1316282072803006e-14\n",
      "progress: 99 w =  2.0000000000000013 loss =  1.262177448353619e-29\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val,y_val in zip(x_data,y_data):\n",
    "        grad= gradient(x_val,y_val)\n",
    "        w = w - 0.01*grad # Learning Rate = aplha = 0.01\n",
    "        print(\"\\tgrad: \", x_val,y_val,grad)\n",
    "        l = losses(x_val,y_val)\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w = \", w, \"loss = \", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict(before training) 4 8.000000000000005\n"
     ]
    }
   ],
   "source": [
    "#After Training\n",
    "print(\"predict(before training)\", 4, forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 32.981964111328125 \n",
      "Epoch: 1 | Loss: 15.191006660461426 \n",
      "Epoch: 2 | Loss: 7.263671398162842 \n",
      "Epoch: 3 | Loss: 3.727447271347046 \n",
      "Epoch: 4 | Loss: 2.1461215019226074 \n",
      "Epoch: 5 | Loss: 1.4351660013198853 \n",
      "Epoch: 6 | Loss: 1.1117733716964722 \n",
      "Epoch: 7 | Loss: 0.9610118865966797 \n",
      "Epoch: 8 | Loss: 0.8871991634368896 \n",
      "Epoch: 9 | Loss: 0.8477379083633423 \n",
      "Epoch: 10 | Loss: 0.8236632347106934 \n",
      "Epoch: 11 | Loss: 0.8065322041511536 \n",
      "Epoch: 12 | Loss: 0.7925849556922913 \n",
      "Epoch: 13 | Loss: 0.7801446318626404 \n",
      "Epoch: 14 | Loss: 0.7684659957885742 \n",
      "Epoch: 15 | Loss: 0.757213830947876 \n",
      "Epoch: 16 | Loss: 0.7462394833564758 \n",
      "Epoch: 17 | Loss: 0.735473096370697 \n",
      "Epoch: 18 | Loss: 0.7248849868774414 \n",
      "Epoch: 19 | Loss: 0.7144590616226196 \n",
      "Epoch: 20 | Loss: 0.7041878700256348 \n",
      "Epoch: 21 | Loss: 0.6940654516220093 \n",
      "Epoch: 22 | Loss: 0.6840903759002686 \n",
      "Epoch: 23 | Loss: 0.6742585301399231 \n",
      "Epoch: 24 | Loss: 0.6645679473876953 \n",
      "Epoch: 25 | Loss: 0.6550168991088867 \n",
      "Epoch: 26 | Loss: 0.6456035375595093 \n",
      "Epoch: 27 | Loss: 0.6363252997398376 \n",
      "Epoch: 28 | Loss: 0.6271804571151733 \n",
      "Epoch: 29 | Loss: 0.6181665658950806 \n",
      "Epoch: 30 | Loss: 0.6092824935913086 \n",
      "Epoch: 31 | Loss: 0.600525975227356 \n",
      "Epoch: 32 | Loss: 0.591895580291748 \n",
      "Epoch: 33 | Loss: 0.5833892226219177 \n",
      "Epoch: 34 | Loss: 0.5750049948692322 \n",
      "Epoch: 35 | Loss: 0.5667414665222168 \n",
      "Epoch: 36 | Loss: 0.5585964918136597 \n",
      "Epoch: 37 | Loss: 0.5505681037902832 \n",
      "Epoch: 38 | Loss: 0.542655885219574 \n",
      "Epoch: 39 | Loss: 0.5348573923110962 \n",
      "Epoch: 40 | Loss: 0.5271701812744141 \n",
      "Epoch: 41 | Loss: 0.5195937752723694 \n",
      "Epoch: 42 | Loss: 0.5121263265609741 \n",
      "Epoch: 43 | Loss: 0.5047662258148193 \n",
      "Epoch: 44 | Loss: 0.49751222133636475 \n",
      "Epoch: 45 | Loss: 0.49036210775375366 \n",
      "Epoch: 46 | Loss: 0.4833146929740906 \n",
      "Epoch: 47 | Loss: 0.47636881470680237 \n",
      "Epoch: 48 | Loss: 0.46952247619628906 \n",
      "Epoch: 49 | Loss: 0.46277451515197754 \n",
      "Epoch: 50 | Loss: 0.4561244547367096 \n",
      "Epoch: 51 | Loss: 0.44956883788108826 \n",
      "Epoch: 52 | Loss: 0.4431077837944031 \n",
      "Epoch: 53 | Loss: 0.4367395043373108 \n",
      "Epoch: 54 | Loss: 0.4304628074169159 \n",
      "Epoch: 55 | Loss: 0.4242766201496124 \n",
      "Epoch: 56 | Loss: 0.4181790053844452 \n",
      "Epoch: 57 | Loss: 0.41216903924942017 \n",
      "Epoch: 58 | Loss: 0.40624549984931946 \n",
      "Epoch: 59 | Loss: 0.40040722489356995 \n",
      "Epoch: 60 | Loss: 0.39465296268463135 \n",
      "Epoch: 61 | Loss: 0.3889811933040619 \n",
      "Epoch: 62 | Loss: 0.3833908140659332 \n",
      "Epoch: 63 | Loss: 0.37788084149360657 \n",
      "Epoch: 64 | Loss: 0.3724501132965088 \n",
      "Epoch: 65 | Loss: 0.3670971393585205 \n",
      "Epoch: 66 | Loss: 0.3618216812610626 \n",
      "Epoch: 67 | Loss: 0.3566214144229889 \n",
      "Epoch: 68 | Loss: 0.3514966666698456 \n",
      "Epoch: 69 | Loss: 0.3464449346065521 \n",
      "Epoch: 70 | Loss: 0.34146615862846375 \n",
      "Epoch: 71 | Loss: 0.3365585207939148 \n",
      "Epoch: 72 | Loss: 0.33172154426574707 \n",
      "Epoch: 73 | Loss: 0.3269541561603546 \n",
      "Epoch: 74 | Loss: 0.3222554624080658 \n",
      "Epoch: 75 | Loss: 0.317624032497406 \n",
      "Epoch: 76 | Loss: 0.31305915117263794 \n",
      "Epoch: 77 | Loss: 0.30856019258499146 \n",
      "Epoch: 78 | Loss: 0.30412566661834717 \n",
      "Epoch: 79 | Loss: 0.299754798412323 \n",
      "Epoch: 80 | Loss: 0.29544728994369507 \n",
      "Epoch: 81 | Loss: 0.2912009656429291 \n",
      "Epoch: 82 | Loss: 0.2870160937309265 \n",
      "Epoch: 83 | Loss: 0.28289103507995605 \n",
      "Epoch: 84 | Loss: 0.27882546186447144 \n",
      "Epoch: 85 | Loss: 0.2748182415962219 \n",
      "Epoch: 86 | Loss: 0.2708687484264374 \n",
      "Epoch: 87 | Loss: 0.26697593927383423 \n",
      "Epoch: 88 | Loss: 0.26313889026641846 \n",
      "Epoch: 89 | Loss: 0.2593573331832886 \n",
      "Epoch: 90 | Loss: 0.2556299567222595 \n",
      "Epoch: 91 | Loss: 0.25195634365081787 \n",
      "Epoch: 92 | Loss: 0.24833525717258453 \n",
      "Epoch: 93 | Loss: 0.2447662055492401 \n",
      "Epoch: 94 | Loss: 0.24124836921691895 \n",
      "Epoch: 95 | Loss: 0.23778122663497925 \n",
      "Epoch: 96 | Loss: 0.23436424136161804 \n",
      "Epoch: 97 | Loss: 0.23099616169929504 \n",
      "Epoch: 98 | Loss: 0.22767607867717743 \n",
      "Epoch: 99 | Loss: 0.22440415620803833 \n",
      "Epoch: 100 | Loss: 0.2211790233850479 \n",
      "Epoch: 101 | Loss: 0.21800027787685394 \n",
      "Epoch: 102 | Loss: 0.21486720442771912 \n",
      "Epoch: 103 | Loss: 0.21177931129932404 \n",
      "Epoch: 104 | Loss: 0.2087356448173523 \n",
      "Epoch: 105 | Loss: 0.20573608577251434 \n",
      "Epoch: 106 | Loss: 0.20277906954288483 \n",
      "Epoch: 107 | Loss: 0.19986477494239807 \n",
      "Epoch: 108 | Loss: 0.19699232280254364 \n",
      "Epoch: 109 | Loss: 0.19416137039661407 \n",
      "Epoch: 110 | Loss: 0.19137084484100342 \n",
      "Epoch: 111 | Loss: 0.18862071633338928 \n",
      "Epoch: 112 | Loss: 0.1859099566936493 \n",
      "Epoch: 113 | Loss: 0.18323785066604614 \n",
      "Epoch: 114 | Loss: 0.1806047409772873 \n",
      "Epoch: 115 | Loss: 0.17800912261009216 \n",
      "Epoch: 116 | Loss: 0.17545080184936523 \n",
      "Epoch: 117 | Loss: 0.17292924225330353 \n",
      "Epoch: 118 | Loss: 0.1704440861940384 \n",
      "Epoch: 119 | Loss: 0.16799449920654297 \n",
      "Epoch: 120 | Loss: 0.16558018326759338 \n",
      "Epoch: 121 | Loss: 0.1632004827260971 \n",
      "Epoch: 122 | Loss: 0.1608550101518631 \n",
      "Epoch: 123 | Loss: 0.15854336321353912 \n",
      "Epoch: 124 | Loss: 0.15626464784145355 \n",
      "Epoch: 125 | Loss: 0.15401899814605713 \n",
      "Epoch: 126 | Loss: 0.15180562436580658 \n",
      "Epoch: 127 | Loss: 0.14962390065193176 \n",
      "Epoch: 128 | Loss: 0.1474735587835312 \n",
      "Epoch: 129 | Loss: 0.14535391330718994 \n",
      "Epoch: 130 | Loss: 0.14326515793800354 \n",
      "Epoch: 131 | Loss: 0.14120616018772125 \n",
      "Epoch: 132 | Loss: 0.1391768604516983 \n",
      "Epoch: 133 | Loss: 0.1371765434741974 \n",
      "Epoch: 134 | Loss: 0.13520509004592896 \n",
      "Epoch: 135 | Loss: 0.1332620233297348 \n",
      "Epoch: 136 | Loss: 0.13134685158729553 \n",
      "Epoch: 137 | Loss: 0.1294591724872589 \n",
      "Epoch: 138 | Loss: 0.12759865820407867 \n",
      "Epoch: 139 | Loss: 0.1257648468017578 \n",
      "Epoch: 140 | Loss: 0.12395729869604111 \n",
      "Epoch: 141 | Loss: 0.12217610329389572 \n",
      "Epoch: 142 | Loss: 0.12042009830474854 \n",
      "Epoch: 143 | Loss: 0.11868953704833984 \n",
      "Epoch: 144 | Loss: 0.11698373407125473 \n",
      "Epoch: 145 | Loss: 0.11530247330665588 \n",
      "Epoch: 146 | Loss: 0.11364555358886719 \n",
      "Epoch: 147 | Loss: 0.11201211810112 \n",
      "Epoch: 148 | Loss: 0.11040222644805908 \n",
      "Epoch: 149 | Loss: 0.10881568491458893 \n",
      "Epoch: 150 | Loss: 0.10725177824497223 \n",
      "Epoch: 151 | Loss: 0.1057104766368866 \n",
      "Epoch: 152 | Loss: 0.10419121384620667 \n",
      "Epoch: 153 | Loss: 0.10269374400377274 \n",
      "Epoch: 154 | Loss: 0.10121787339448929 \n",
      "Epoch: 155 | Loss: 0.09976321458816528 \n",
      "Epoch: 156 | Loss: 0.09832963347434998 \n",
      "Epoch: 157 | Loss: 0.09691639244556427 \n",
      "Epoch: 158 | Loss: 0.09552344679832458 \n",
      "Epoch: 159 | Loss: 0.09415076673030853 \n",
      "Epoch: 160 | Loss: 0.0927976593375206 \n",
      "Epoch: 161 | Loss: 0.09146393090486526 \n",
      "Epoch: 162 | Loss: 0.09014950692653656 \n",
      "Epoch: 163 | Loss: 0.08885381370782852 \n",
      "Epoch: 164 | Loss: 0.08757681399583817 \n",
      "Epoch: 165 | Loss: 0.08631844073534012 \n",
      "Epoch: 166 | Loss: 0.08507770299911499 \n",
      "Epoch: 167 | Loss: 0.0838550478219986 \n",
      "Epoch: 168 | Loss: 0.08264999091625214 \n",
      "Epoch: 169 | Loss: 0.08146213740110397 \n",
      "Epoch: 170 | Loss: 0.08029134571552277 \n",
      "Epoch: 171 | Loss: 0.07913748919963837 \n",
      "Epoch: 172 | Loss: 0.07800020277500153 \n",
      "Epoch: 173 | Loss: 0.07687915116548538 \n",
      "Epoch: 174 | Loss: 0.07577440142631531 \n",
      "Epoch: 175 | Loss: 0.07468530535697937 \n",
      "Epoch: 176 | Loss: 0.07361193001270294 \n",
      "Epoch: 177 | Loss: 0.07255395501852036 \n",
      "Epoch: 178 | Loss: 0.07151132822036743 \n",
      "Epoch: 179 | Loss: 0.0704835057258606 \n",
      "Epoch: 180 | Loss: 0.06947053968906403 \n",
      "Epoch: 181 | Loss: 0.06847217679023743 \n",
      "Epoch: 182 | Loss: 0.06748807430267334 \n",
      "Epoch: 183 | Loss: 0.06651818007230759 \n",
      "Epoch: 184 | Loss: 0.06556227058172226 \n",
      "Epoch: 185 | Loss: 0.0646199956536293 \n",
      "Epoch: 186 | Loss: 0.0636913850903511 \n",
      "Epoch: 187 | Loss: 0.06277598440647125 \n",
      "Epoch: 188 | Loss: 0.06187383830547333 \n",
      "Epoch: 189 | Loss: 0.06098466366529465 \n",
      "Epoch: 190 | Loss: 0.06010809540748596 \n",
      "Epoch: 191 | Loss: 0.05924428999423981 \n",
      "Epoch: 192 | Loss: 0.05839287117123604 \n",
      "Epoch: 193 | Loss: 0.0575537234544754 \n",
      "Epoch: 194 | Loss: 0.0567266047000885 \n",
      "Epoch: 195 | Loss: 0.055911291390657425 \n",
      "Epoch: 196 | Loss: 0.055107660591602325 \n",
      "Epoch: 197 | Loss: 0.054315775632858276 \n",
      "Epoch: 198 | Loss: 0.05353512614965439 \n",
      "Epoch: 199 | Loss: 0.052765730768442154 \n",
      "Epoch: 200 | Loss: 0.052007418125867844 \n",
      "Epoch: 201 | Loss: 0.05126003548502922 \n",
      "Epoch: 202 | Loss: 0.050523340702056885 \n",
      "Epoch: 203 | Loss: 0.049797192215919495 \n",
      "Epoch: 204 | Loss: 0.04908161610364914 \n",
      "Epoch: 205 | Loss: 0.04837627708911896 \n",
      "Epoch: 206 | Loss: 0.047680940479040146 \n",
      "Epoch: 207 | Loss: 0.04699566215276718 \n",
      "Epoch: 208 | Loss: 0.04632028937339783 \n",
      "Epoch: 209 | Loss: 0.04565465450286865 \n",
      "Epoch: 210 | Loss: 0.044998422265052795 \n",
      "Epoch: 211 | Loss: 0.04435179755091667 \n",
      "Epoch: 212 | Loss: 0.043714478611946106 \n",
      "Epoch: 213 | Loss: 0.04308612272143364 \n",
      "Epoch: 214 | Loss: 0.04246686398983002 \n",
      "Epoch: 215 | Loss: 0.04185653477907181 \n",
      "Epoch: 216 | Loss: 0.04125504195690155 \n",
      "Epoch: 217 | Loss: 0.040662121027708054 \n",
      "Epoch: 218 | Loss: 0.04007773473858833 \n",
      "Epoch: 219 | Loss: 0.03950180485844612 \n",
      "Epoch: 220 | Loss: 0.03893406689167023 \n",
      "Epoch: 221 | Loss: 0.038374532014131546 \n",
      "Epoch: 222 | Loss: 0.03782312572002411 \n",
      "Epoch: 223 | Loss: 0.03727952018380165 \n",
      "Epoch: 224 | Loss: 0.0367436558008194 \n",
      "Epoch: 225 | Loss: 0.03621558099985123 \n",
      "Epoch: 226 | Loss: 0.0356951504945755 \n",
      "Epoch: 227 | Loss: 0.03518211469054222 \n",
      "Epoch: 228 | Loss: 0.034676652401685715 \n",
      "Epoch: 229 | Loss: 0.034178227186203 \n",
      "Epoch: 230 | Loss: 0.0336870476603508 \n",
      "Epoch: 231 | Loss: 0.03320282697677612 \n",
      "Epoch: 232 | Loss: 0.03272563964128494 \n",
      "Epoch: 233 | Loss: 0.032255399972200394 \n",
      "Epoch: 234 | Loss: 0.031791768968105316 \n",
      "Epoch: 235 | Loss: 0.031334955245256424 \n",
      "Epoch: 236 | Loss: 0.030884549021720886 \n",
      "Epoch: 237 | Loss: 0.030440673232078552 \n",
      "Epoch: 238 | Loss: 0.030003271996974945 \n",
      "Epoch: 239 | Loss: 0.02957199700176716 \n",
      "Epoch: 240 | Loss: 0.029147028923034668 \n",
      "Epoch: 241 | Loss: 0.028728174045681953 \n",
      "Epoch: 242 | Loss: 0.028315283358097076 \n",
      "Epoch: 243 | Loss: 0.027908403426408768 \n",
      "Epoch: 244 | Loss: 0.02750731259584427 \n",
      "Epoch: 245 | Loss: 0.02711196057498455 \n",
      "Epoch: 246 | Loss: 0.026722321286797523 \n",
      "Epoch: 247 | Loss: 0.02633824571967125 \n",
      "Epoch: 248 | Loss: 0.025959743186831474 \n",
      "Epoch: 249 | Loss: 0.02558661252260208 \n",
      "Epoch: 250 | Loss: 0.02521897293627262 \n",
      "Epoch: 251 | Loss: 0.02485644817352295 \n",
      "Epoch: 252 | Loss: 0.024499259889125824 \n",
      "Epoch: 253 | Loss: 0.02414715103805065 \n",
      "Epoch: 254 | Loss: 0.023800093680620193 \n",
      "Epoch: 255 | Loss: 0.02345805987715721 \n",
      "Epoch: 256 | Loss: 0.023120960220694542 \n",
      "Epoch: 257 | Loss: 0.022788632661104202 \n",
      "Epoch: 258 | Loss: 0.02246115356683731 \n",
      "Epoch: 259 | Loss: 0.022138362750411034 \n",
      "Epoch: 260 | Loss: 0.02182021737098694 \n",
      "Epoch: 261 | Loss: 0.02150658331811428 \n",
      "Epoch: 262 | Loss: 0.02119753137230873 \n",
      "Epoch: 263 | Loss: 0.020892884582281113 \n",
      "Epoch: 264 | Loss: 0.02059258706867695 \n",
      "Epoch: 265 | Loss: 0.020296674221754074 \n",
      "Epoch: 266 | Loss: 0.02000492624938488 \n",
      "Epoch: 267 | Loss: 0.019717518240213394 \n",
      "Epoch: 268 | Loss: 0.019434088841080666 \n",
      "Epoch: 269 | Loss: 0.019154777750372887 \n",
      "Epoch: 270 | Loss: 0.018879510462284088 \n",
      "Epoch: 271 | Loss: 0.018608249723911285 \n",
      "Epoch: 272 | Loss: 0.018340740352869034 \n",
      "Epoch: 273 | Loss: 0.018077176064252853 \n",
      "Epoch: 274 | Loss: 0.01781735010445118 \n",
      "Epoch: 275 | Loss: 0.017561346292495728 \n",
      "Epoch: 276 | Loss: 0.017308970913290977 \n",
      "Epoch: 277 | Loss: 0.0170601699501276 \n",
      "Epoch: 278 | Loss: 0.016814978793263435 \n",
      "Epoch: 279 | Loss: 0.016573365777730942 \n",
      "Epoch: 280 | Loss: 0.016335204243659973 \n",
      "Epoch: 281 | Loss: 0.01610037311911583 \n",
      "Epoch: 282 | Loss: 0.01586895063519478 \n",
      "Epoch: 283 | Loss: 0.015640966594219208 \n",
      "Epoch: 284 | Loss: 0.015416115522384644 \n",
      "Epoch: 285 | Loss: 0.015194598585367203 \n",
      "Epoch: 286 | Loss: 0.01497622113674879 \n",
      "Epoch: 287 | Loss: 0.014761019498109818 \n",
      "Epoch: 288 | Loss: 0.014548877254128456 \n",
      "Epoch: 289 | Loss: 0.014339739456772804 \n",
      "Epoch: 290 | Loss: 0.014133663848042488 \n",
      "Epoch: 291 | Loss: 0.013930520974099636 \n",
      "Epoch: 292 | Loss: 0.013730375096201897 \n",
      "Epoch: 293 | Loss: 0.013533029705286026 \n",
      "Epoch: 294 | Loss: 0.013338537886738777 \n",
      "Epoch: 295 | Loss: 0.013146885670721531 \n",
      "Epoch: 296 | Loss: 0.012957903556525707 \n",
      "Epoch: 297 | Loss: 0.01277170330286026 \n",
      "Epoch: 298 | Loss: 0.012588084675371647 \n",
      "Epoch: 299 | Loss: 0.012407245114445686 \n",
      "Epoch: 300 | Loss: 0.012228929437696934 \n",
      "Epoch: 301 | Loss: 0.012053117156028748 \n",
      "Epoch: 302 | Loss: 0.011879922822117805 \n",
      "Epoch: 303 | Loss: 0.011709213256835938 \n",
      "Epoch: 304 | Loss: 0.011540969833731651 \n",
      "Epoch: 305 | Loss: 0.011375035159289837 \n",
      "Epoch: 306 | Loss: 0.011211570352315903 \n",
      "Epoch: 307 | Loss: 0.011050432920455933 \n",
      "Epoch: 308 | Loss: 0.01089165173470974 \n",
      "Epoch: 309 | Loss: 0.01073510106652975 \n",
      "Epoch: 310 | Loss: 0.010580839589238167 \n",
      "Epoch: 311 | Loss: 0.010428763926029205 \n",
      "Epoch: 312 | Loss: 0.010278884321451187 \n",
      "Epoch: 313 | Loss: 0.01013118401169777 \n",
      "Epoch: 314 | Loss: 0.009985553100705147 \n",
      "Epoch: 315 | Loss: 0.00984206609427929 \n",
      "Epoch: 316 | Loss: 0.009700573980808258 \n",
      "Epoch: 317 | Loss: 0.009561225771903992 \n",
      "Epoch: 318 | Loss: 0.009423828683793545 \n",
      "Epoch: 319 | Loss: 0.009288334287703037 \n",
      "Epoch: 320 | Loss: 0.009154899045825005 \n",
      "Epoch: 321 | Loss: 0.009023286402225494 \n",
      "Epoch: 322 | Loss: 0.008893663063645363 \n",
      "Epoch: 323 | Loss: 0.008765793405473232 \n",
      "Epoch: 324 | Loss: 0.008639822714030743 \n",
      "Epoch: 325 | Loss: 0.008515632711350918 \n",
      "Epoch: 326 | Loss: 0.008393301628530025 \n",
      "Epoch: 327 | Loss: 0.008272642269730568 \n",
      "Epoch: 328 | Loss: 0.008153801783919334 \n",
      "Epoch: 329 | Loss: 0.008036562241613865 \n",
      "Epoch: 330 | Loss: 0.007921055890619755 \n",
      "Epoch: 331 | Loss: 0.0078072333708405495 \n",
      "Epoch: 332 | Loss: 0.007695045787841082 \n",
      "Epoch: 333 | Loss: 0.007584419101476669 \n",
      "Epoch: 334 | Loss: 0.007475423626601696 \n",
      "Epoch: 335 | Loss: 0.007368029560893774 \n",
      "Epoch: 336 | Loss: 0.007262111641466618 \n",
      "Epoch: 337 | Loss: 0.007157782092690468 \n",
      "Epoch: 338 | Loss: 0.00705492589622736 \n",
      "Epoch: 339 | Loss: 0.0069535160437226295 \n",
      "Epoch: 340 | Loss: 0.006853548809885979 \n",
      "Epoch: 341 | Loss: 0.0067550549283623695 \n",
      "Epoch: 342 | Loss: 0.006657996214926243 \n",
      "Epoch: 343 | Loss: 0.006562288384884596 \n",
      "Epoch: 344 | Loss: 0.006467989645898342 \n",
      "Epoch: 345 | Loss: 0.006375055760145187 \n",
      "Epoch: 346 | Loss: 0.0062834215350449085 \n",
      "Epoch: 347 | Loss: 0.00619309488683939 \n",
      "Epoch: 348 | Loss: 0.00610407255589962 \n",
      "Epoch: 349 | Loss: 0.0060163941234350204 \n",
      "Epoch: 350 | Loss: 0.005929958075284958 \n",
      "Epoch: 351 | Loss: 0.005844681058079004 \n",
      "Epoch: 352 | Loss: 0.005760710220783949 \n",
      "Epoch: 353 | Loss: 0.005677943117916584 \n",
      "Epoch: 354 | Loss: 0.005596318282186985 \n",
      "Epoch: 355 | Loss: 0.005515871569514275 \n",
      "Epoch: 356 | Loss: 0.005436611827462912 \n",
      "Epoch: 357 | Loss: 0.005358504597097635 \n",
      "Epoch: 358 | Loss: 0.005281461402773857 \n",
      "Epoch: 359 | Loss: 0.005205575376749039 \n",
      "Epoch: 360 | Loss: 0.005130767822265625 \n",
      "Epoch: 361 | Loss: 0.005057027097791433 \n",
      "Epoch: 362 | Loss: 0.004984308034181595 \n",
      "Epoch: 363 | Loss: 0.0049126907251775265 \n",
      "Epoch: 364 | Loss: 0.0048420727252960205 \n",
      "Epoch: 365 | Loss: 0.004772492218762636 \n",
      "Epoch: 366 | Loss: 0.004703938029706478 \n",
      "Epoch: 367 | Loss: 0.0046363272704184055 \n",
      "Epoch: 368 | Loss: 0.004569704644382 \n",
      "Epoch: 369 | Loss: 0.00450400123372674 \n",
      "Epoch: 370 | Loss: 0.004439300857484341 \n",
      "Epoch: 371 | Loss: 0.004375518299639225 \n",
      "Epoch: 372 | Loss: 0.004312601871788502 \n",
      "Epoch: 373 | Loss: 0.004250622354447842 \n",
      "Epoch: 374 | Loss: 0.004189551342278719 \n",
      "Epoch: 375 | Loss: 0.004129332490265369 \n",
      "Epoch: 376 | Loss: 0.004069983959197998 \n",
      "Epoch: 377 | Loss: 0.004011484794318676 \n",
      "Epoch: 378 | Loss: 0.003953825682401657 \n",
      "Epoch: 379 | Loss: 0.003897005459293723 \n",
      "Epoch: 380 | Loss: 0.003841014113277197 \n",
      "Epoch: 381 | Loss: 0.0037857932038605213 \n",
      "Epoch: 382 | Loss: 0.003731394186615944 \n",
      "Epoch: 383 | Loss: 0.0036777667701244354 \n",
      "Epoch: 384 | Loss: 0.003624915610998869 \n",
      "Epoch: 385 | Loss: 0.0035728365182876587 \n",
      "Epoch: 386 | Loss: 0.003521465230733156 \n",
      "Epoch: 387 | Loss: 0.003470884170383215 \n",
      "Epoch: 388 | Loss: 0.003420971566811204 \n",
      "Epoch: 389 | Loss: 0.0033718221820890903 \n",
      "Epoch: 390 | Loss: 0.003323350101709366 \n",
      "Epoch: 391 | Loss: 0.00327560817822814 \n",
      "Epoch: 392 | Loss: 0.0032284995540976524 \n",
      "Epoch: 393 | Loss: 0.003182109212502837 \n",
      "Epoch: 394 | Loss: 0.0031363703310489655 \n",
      "Epoch: 395 | Loss: 0.003091314807534218 \n",
      "Epoch: 396 | Loss: 0.0030468646436929703 \n",
      "Epoch: 397 | Loss: 0.003003109246492386 \n",
      "Epoch: 398 | Loss: 0.002959929406642914 \n",
      "Epoch: 399 | Loss: 0.00291741406545043 \n",
      "Epoch: 400 | Loss: 0.002875484060496092 \n",
      "Epoch: 401 | Loss: 0.002834164071828127 \n",
      "Epoch: 402 | Loss: 0.0027934194076806307 \n",
      "Epoch: 403 | Loss: 0.002753281034529209 \n",
      "Epoch: 404 | Loss: 0.0027137116994708776 \n",
      "Epoch: 405 | Loss: 0.002674711402505636 \n",
      "Epoch: 406 | Loss: 0.0026362473145127296 \n",
      "Epoch: 407 | Loss: 0.0025983729865401983 \n",
      "Epoch: 408 | Loss: 0.002561034634709358 \n",
      "Epoch: 409 | Loss: 0.002524243900552392 \n",
      "Epoch: 410 | Loss: 0.0024879425764083862 \n",
      "Epoch: 411 | Loss: 0.0024522067978978157 \n",
      "Epoch: 412 | Loss: 0.0024169550742954016 \n",
      "Epoch: 413 | Loss: 0.0023822225630283356 \n",
      "Epoch: 414 | Loss: 0.002347949892282486 \n",
      "Epoch: 415 | Loss: 0.002314233221113682 \n",
      "Epoch: 416 | Loss: 0.002280974294990301 \n",
      "Epoch: 417 | Loss: 0.002248198725283146 \n",
      "Epoch: 418 | Loss: 0.0022158902138471603 \n",
      "Epoch: 419 | Loss: 0.002184019424021244 \n",
      "Epoch: 420 | Loss: 0.0021526224445551634 \n",
      "Epoch: 421 | Loss: 0.0021217067260295153 \n",
      "Epoch: 422 | Loss: 0.00209121429361403 \n",
      "Epoch: 423 | Loss: 0.0020611591171473265 \n",
      "Epoch: 424 | Loss: 0.002031543990597129 \n",
      "Epoch: 425 | Loss: 0.0020023516844958067 \n",
      "Epoch: 426 | Loss: 0.001973576843738556 \n",
      "Epoch: 427 | Loss: 0.0019452158594503999 \n",
      "Epoch: 428 | Loss: 0.0019172559259459376 \n",
      "Epoch: 429 | Loss: 0.0018896955298259854 \n",
      "Epoch: 430 | Loss: 0.0018625404918566346 \n",
      "Epoch: 431 | Loss: 0.0018357638036832213 \n",
      "Epoch: 432 | Loss: 0.0018093913095071912 \n",
      "Epoch: 433 | Loss: 0.0017834011232480407 \n",
      "Epoch: 434 | Loss: 0.001757750054821372 \n",
      "Epoch: 435 | Loss: 0.0017324857180938125 \n",
      "Epoch: 436 | Loss: 0.001707581919617951 \n",
      "Epoch: 437 | Loss: 0.0016830350505188107 \n",
      "Epoch: 438 | Loss: 0.0016588671132922173 \n",
      "Epoch: 439 | Loss: 0.0016350268851965666 \n",
      "Epoch: 440 | Loss: 0.001611510873772204 \n",
      "Epoch: 441 | Loss: 0.0015883620362728834 \n",
      "Epoch: 442 | Loss: 0.001565537299029529 \n",
      "Epoch: 443 | Loss: 0.0015430330531671643 \n",
      "Epoch: 444 | Loss: 0.0015208504628390074 \n",
      "Epoch: 445 | Loss: 0.0014990040799602866 \n",
      "Epoch: 446 | Loss: 0.0014774496667087078 \n",
      "Epoch: 447 | Loss: 0.0014562357682734728 \n",
      "Epoch: 448 | Loss: 0.0014353027800098062 \n",
      "Epoch: 449 | Loss: 0.0014146853936836123 \n",
      "Epoch: 450 | Loss: 0.0013943476369604468 \n",
      "Epoch: 451 | Loss: 0.001374299288727343 \n",
      "Epoch: 452 | Loss: 0.0013545630499720573 \n",
      "Epoch: 453 | Loss: 0.0013350904919207096 \n",
      "Epoch: 454 | Loss: 0.001315915142185986 \n",
      "Epoch: 455 | Loss: 0.0012969935778528452 \n",
      "Epoch: 456 | Loss: 0.0012783443089574575 \n",
      "Epoch: 457 | Loss: 0.0012599703622981906 \n",
      "Epoch: 458 | Loss: 0.0012418708065524697 \n",
      "Epoch: 459 | Loss: 0.001224003848619759 \n",
      "Epoch: 460 | Loss: 0.0012064339825883508 \n",
      "Epoch: 461 | Loss: 0.0011891075409948826 \n",
      "Epoch: 462 | Loss: 0.00117200193926692 \n",
      "Epoch: 463 | Loss: 0.0011551486095413566 \n",
      "Epoch: 464 | Loss: 0.0011385508114472032 \n",
      "Epoch: 465 | Loss: 0.0011222074972465634 \n",
      "Epoch: 466 | Loss: 0.0011060654651373625 \n",
      "Epoch: 467 | Loss: 0.0010901556815952063 \n",
      "Epoch: 468 | Loss: 0.0010745153995230794 \n",
      "Epoch: 469 | Loss: 0.0010590601013973355 \n",
      "Epoch: 470 | Loss: 0.0010438445024192333 \n",
      "Epoch: 471 | Loss: 0.0010288397315889597 \n",
      "Epoch: 472 | Loss: 0.0010140639496967196 \n",
      "Epoch: 473 | Loss: 0.000999492360278964 \n",
      "Epoch: 474 | Loss: 0.0009851193754002452 \n",
      "Epoch: 475 | Loss: 0.0009709465084597468 \n",
      "Epoch: 476 | Loss: 0.0009569949470460415 \n",
      "Epoch: 477 | Loss: 0.0009432569495402277 \n",
      "Epoch: 478 | Loss: 0.0009297087090089917 \n",
      "Epoch: 479 | Loss: 0.0009163341019302607 \n",
      "Epoch: 480 | Loss: 0.0009031629888340831 \n",
      "Epoch: 481 | Loss: 0.0008901930414140224 \n",
      "Epoch: 482 | Loss: 0.0008773944573476911 \n",
      "Epoch: 483 | Loss: 0.0008647757931612432 \n",
      "Epoch: 484 | Loss: 0.0008523468859493732 \n",
      "Epoch: 485 | Loss: 0.0008400989463552833 \n",
      "Epoch: 486 | Loss: 0.0008280299953185022 \n",
      "Epoch: 487 | Loss: 0.0008161314763128757 \n",
      "Epoch: 488 | Loss: 0.0008044081041589379 \n",
      "Epoch: 489 | Loss: 0.0007928513805381954 \n",
      "Epoch: 490 | Loss: 0.0007814530399627984 \n",
      "Epoch: 491 | Loss: 0.0007702114526182413 \n",
      "Epoch: 492 | Loss: 0.0007591458270326257 \n",
      "Epoch: 493 | Loss: 0.0007482445798814297 \n",
      "Epoch: 494 | Loss: 0.0007374917040579021 \n",
      "Epoch: 495 | Loss: 0.000726887141354382 \n",
      "Epoch: 496 | Loss: 0.000716448063030839 \n",
      "Epoch: 497 | Loss: 0.0007061462383717299 \n",
      "Epoch: 498 | Loss: 0.0006959897000342607 \n",
      "Epoch: 499 | Loss: 0.0006859858985990286 \n"
     ]
    }
   ],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (after training) 4 7.976367950439453\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "hour_var = tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
